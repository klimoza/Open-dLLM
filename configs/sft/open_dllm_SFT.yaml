model:
  model_path: fredzzp/open-dcoder-0.5B
  attn_implementation: flash_attention_2

data:
  train_path: ./opc-sft-stage2/educational_instruct #./openai_humaneval/openai_humaneval #./open_code_instruct/data  # Updated to local path
  train_size: 30_000_000 # 1T tokens
  dataloader_type: native
  data_type: sft_data
  max_seq_len: 768 # 4096
  prompt_keys: instruction
  text_keys: output
  drop_last: true
  datasets_type: iterable # mapping
  prefetch_factor: 2
  num_workers: 1 #2

train:
  output_dir: logs/Open_DLLM_SFT
  data_parallel_mode: ddp
  ulysses_parallel_size: 1
  global_batch_size: 512 #512
  micro_batch_size: 16 #16
  rmpad: false
  rmpad_with_pos_ids: true
  enable_masking: true
  bsz_warmup_ratio: 0.00
  dyn_bsz_margin: 0
  dyn_bsz_buffer_size: 100
  optimizer: adamw
  lr: 1.0e-4
  lr_decay_style: cosine
  lr_warmup_ratio: 0.02 
  lr_decay_ratio: 1.0  # Use full training for decay
  lr_min: 1e-6        # Minimum LR
  num_train_epochs: 60
  weight_decay: 0.01
  max_grad_norm: 1.0
  enable_mixed_precision: false
  enable_gradient_checkpointing: false
  enable_full_shard: false
  enable_fsdp_offload: false
  enable_activation_offload: false
  init_device: cuda
  enable_full_determinism: false
  empty_cache_steps: 1000
  ckpt_manager: bytecheckpoint
  load_checkpoint_path: ""
  save_steps: 100000000000
  save_epochs: 5
  save_hf_weights: true
  wandb_project: Open_DLLM_SFT
  wandb_name: Open_DLLM_SFT-opc-sft-stage2-educational_instruct
  use_wandb: true
